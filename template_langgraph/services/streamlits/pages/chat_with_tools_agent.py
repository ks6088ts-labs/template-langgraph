import os
import sqlite3
import tempfile
import uuid
from base64 import b64encode
from dataclasses import dataclass
from enum import Enum

import streamlit as st
from audio_recorder_streamlit import audio_recorder
from langchain_community.callbacks.streamlit import (
    StreamlitCallbackHandler,
)
from langfuse.langchain import CallbackHandler
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.store.sqlite import SqliteStore
from langgraph_checkpoint_cosmosdb import CosmosDBSaver

from template_langgraph.agents.chat_with_tools_agent.agent import (
    AgentState,
    ChatWithToolsAgent,
)
from template_langgraph.loggers import get_logger
from template_langgraph.speeches.stt import SttWrapper
from template_langgraph.speeches.tts import TtsWrapper
from template_langgraph.tools.common import get_default_tools

logger = get_logger(__name__)
logger.setLevel("DEBUG")


class CheckpointType(str, Enum):
    SQLITE = "sqlite"
    COSMOSDB = "cosmosdb"
    MEMORY = "memory"
    NONE = "none"


DEFAULT_CHECKPOINT_TYPE = CheckpointType.NONE
CHECKPOINT_LABELS = {
    CheckpointType.COSMOSDB.value: "Cosmos DB",
    CheckpointType.SQLITE.value: "SQLite",
    CheckpointType.MEMORY.value: "ãƒ¡ãƒ¢ãƒª",
    CheckpointType.NONE.value: "ãªã—",
}


store_conn = sqlite3.connect("store.sqlite", check_same_thread=False)
# thread_id ã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã§ä¿æŒã—ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆåˆ©ç”¨æ™‚ã«æ—¢å­˜ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å†é–‹ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
if "thread_id" not in st.session_state:
    st.session_state["thread_id"] = str(uuid.uuid4())


def image_to_base64(image_bytes: bytes) -> str:
    return b64encode(image_bytes).decode("utf-8")


@st.cache_resource(show_spinner=False)
def load_stt_wrapper(model_size: str = "base"):
    """Load and cache the STT model."""
    stt_wrapper = SttWrapper()
    stt_wrapper.load_model(model_size)
    return stt_wrapper


@dataclass(slots=True)
class AudioSettings:
    audio_bytes: bytes | None
    whisper_model: str
    transcription_language: str
    tts_language: str
    tts_speed: float
    tts_pitch: int
    tts_volume: int


@dataclass(slots=True)
class UserSubmission:
    content: str
    display_items: list[dict[str, object]]

    def to_history_message(self) -> dict[str, object]:
        message: dict[str, object] = {"role": "user", "content": self.content}
        if self.display_items:
            message["attachments"] = self.display_items
        return message


def ensure_session_state_defaults(tool_names: list[str]) -> None:
    st.session_state.setdefault("input_output_mode", "ãƒ†ã‚­ã‚¹ãƒˆ")
    st.session_state.setdefault("selected_tool_names", tool_names)
    st.session_state.setdefault("checkpoint_type", DEFAULT_CHECKPOINT_TYPE.value)


def get_selected_checkpoint_type() -> CheckpointType:
    raw_value = st.session_state.get("checkpoint_type", DEFAULT_CHECKPOINT_TYPE.value)
    try:
        checkpoint = CheckpointType(raw_value)
    except ValueError:
        st.session_state["checkpoint_type"] = DEFAULT_CHECKPOINT_TYPE.value
        return DEFAULT_CHECKPOINT_TYPE
    return checkpoint


def get_checkpointer():
    checkpoint_type = get_selected_checkpoint_type()
    if checkpoint_type is CheckpointType.SQLITE:
        conn = sqlite3.connect("checkpoints.sqlite", check_same_thread=False)
        return SqliteSaver(conn=conn)
    if checkpoint_type is CheckpointType.COSMOSDB:
        from template_langgraph.tools.cosmosdb_tool import get_cosmosdb_settings

        settings = get_cosmosdb_settings()
        os.environ["COSMOSDB_ENDPOINT"] = settings.cosmosdb_host
        os.environ["COSMOSDB_KEY"] = settings.cosmosdb_key

        return CosmosDBSaver(
            database_name=settings.cosmosdb_database_name,
            container_name="checkpoints",
        )
    if checkpoint_type is CheckpointType.MEMORY:
        return InMemorySaver()
    return None


def ensure_agent_graph(selected_tools: list) -> None:
    signature = (tuple(tool.name for tool in selected_tools), get_selected_checkpoint_type().value)
    graph_signature = st.session_state.get("graph_tools_signature")
    if "graph" not in st.session_state or graph_signature != signature:
        st.session_state["graph"] = ChatWithToolsAgent(
            tools=selected_tools,
            checkpointer=get_checkpointer(),
            store=SqliteStore(
                conn=store_conn,
            ),
        ).create_graph()
        st.session_state["graph_tools_signature"] = signature


def _list_existing_thread_ids() -> list[str]:
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ã‚¿ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ thread_id ã‚’åˆ—æŒ™ (æœ€å¤§50ä»¶)ã€‚"""
    checkpointer = get_checkpointer()
    if not checkpointer:
        return []
    thread_ids: set[str] = set()
    try:
        for i, snapshot in enumerate(checkpointer.list(config=None)):
            if i > 1000:  # å¿µã®ãŸã‚ç„¡é™å¢—åŠ é˜²æ­¢
                break
            cfg = getattr(snapshot, "config", {}) or {}
            configurable = cfg.get("configurable", {}) if isinstance(cfg, dict) else {}
            tid = configurable.get("thread_id")
            if isinstance(tid, str):
                thread_ids.add(tid)
    except Exception as exc:  # noqa: BLE001
        logger.debug(f"thread list å–å¾—å¤±æ•—: {exc}")
    # ç›´è¿‘åˆ©ç”¨ã‚’å„ªå…ˆã§ãã‚‹æƒ…å ±ãŒç„¡ã„ã®ã§å˜ç´”ã‚½ãƒ¼ãƒˆ
    return sorted(thread_ids)[:50]


def build_sidebar() -> tuple[str, AudioSettings | None]:
    audio_settings: AudioSettings | None = None

    with st.sidebar:
        st.subheader("å…¥å‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰")

        available_tools = get_default_tools()
        tool_name_to_obj = {tool.name: tool for tool in available_tools}
        tool_names = list(tool_name_to_obj.keys())

        ensure_session_state_defaults(tool_names)

        input_mode = st.radio(
            "ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„",
            options=["ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"],
            index=0 if st.session_state["input_output_mode"] == "ãƒ†ã‚­ã‚¹ãƒˆ" else 1,
            help="ãƒ†ã‚­ã‚¹ãƒˆ: å¾“æ¥ã®ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›/å‡ºåŠ›, éŸ³å£°: ãƒã‚¤ã‚¯å…¥åŠ›/éŸ³å£°å‡ºåŠ›",
        )
        st.session_state["input_output_mode"] = input_mode

        if input_mode == "éŸ³å£°":
            audio_settings = render_audio_controls()

        st.divider()
        st.subheader("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ")

        checkpoint_options = [checkpoint.value for checkpoint in CheckpointType]
        current_checkpoint_value = st.session_state["checkpoint_type"]
        if current_checkpoint_value not in checkpoint_options:
            current_checkpoint_value = DEFAULT_CHECKPOINT_TYPE.value
            st.session_state["checkpoint_type"] = current_checkpoint_value
        checkpoint_index = checkpoint_options.index(current_checkpoint_value)
        selected_checkpoint_value = st.selectbox(
            "ä¿å­˜æ–¹æ³•",
            options=checkpoint_options,
            index=checkpoint_index,
            format_func=lambda value: CHECKPOINT_LABELS.get(value, value),
        )
        st.session_state["checkpoint_type"] = selected_checkpoint_value

        # ã‚¹ãƒ¬ãƒƒãƒ‰é¸æŠ UI ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæœ‰åŠ¹æ™‚ã®ã¿ï¼‰
        if get_selected_checkpoint_type() is not CheckpointType.NONE:
            existing_threads = _list_existing_thread_ids()
            st.subheader("ã‚¹ãƒ¬ãƒƒãƒ‰")
            new_label = "<æ–°è¦ä½œæˆ>"
            options = [new_label, *existing_threads]
            current_thread = st.session_state.get("thread_id")
            # æ—¢å­˜ã«ä¸€è‡´ã™ã‚‹ãªã‚‰ãã® indexã€ãªã‘ã‚Œã° 0 (æ–°è¦)
            if current_thread in existing_threads:
                default_index = options.index(current_thread)
            else:
                default_index = 0
            selected = st.selectbox("æ—¢å­˜ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é¸æŠ", options=options, index=default_index)
            if selected == new_label:
                if st.button("ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ç”Ÿæˆ", use_container_width=True):
                    st.session_state["thread_id"] = str(uuid.uuid4())
                    st.experimental_rerun()
            else:
                st.session_state["thread_id"] = selected
            st.caption(f"ç¾åœ¨ã® thread_id: {st.session_state['thread_id']}")

        st.divider()
        st.subheader("ä½¿ç”¨ã™ã‚‹ãƒ„ãƒ¼ãƒ«")

        selected_tool_names = st.multiselect(
            "æœ‰åŠ¹åŒ–ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’é¸æŠ",
            options=tool_names,
            default=st.session_state["selected_tool_names"],
        )
        st.session_state["selected_tool_names"] = selected_tool_names

        selected_tools = [tool_name_to_obj[name] for name in selected_tool_names]
        ensure_agent_graph(selected_tools)

        st.caption("é¸æŠä¸­: " + (", ".join(selected_tool_names) if selected_tool_names else "ãªã—"))

    return input_mode, audio_settings


def render_audio_controls() -> AudioSettings:
    st.subheader("éŸ³å£°èªè­˜è¨­å®š (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")
    audio_bytes = audio_recorder(
        text="ã‚¯ãƒªãƒƒã‚¯ã—ã¦éŸ³å£°å…¥åŠ›ğŸ‘‰ï¸",
        recording_color="red",
        neutral_color="gray",
        icon_name="microphone",
        icon_size="2x",
        key="audio_input",
    )
    whisper_model = st.sidebar.selectbox(
        "Whisperãƒ¢ãƒ‡ãƒ«",
        ["tiny", "base", "small", "medium", "large"],
        index=1,
    )
    transcription_language = st.sidebar.selectbox(
        "æ–‡å­—èµ·ã“ã—è¨€èª",
        ["auto", "ja", "en"],
        index=0,
        help="autoã¯è¨€èªè‡ªå‹•åˆ¤å®šã§ã™",
    )
    tts_language = st.sidebar.selectbox(
        "TTSè¨€èª",
        ["ja", "en", "fr", "de", "ko", "zh-CN"],
        index=0,
    )
    tts_speed = st.sidebar.slider("å†ç”Ÿé€Ÿåº¦", min_value=0.5, max_value=2.0, step=0.1, value=1.0)
    tts_pitch = st.sidebar.slider("ãƒ”ãƒƒãƒ (åŠéŸ³)", min_value=-12, max_value=12, value=0)
    tts_volume = st.sidebar.slider("éŸ³é‡ (dB)", min_value=-20, max_value=10, value=0)

    return AudioSettings(
        audio_bytes=audio_bytes,
        whisper_model=whisper_model,
        transcription_language=transcription_language,
        tts_language=tts_language,
        tts_speed=tts_speed,
        tts_pitch=tts_pitch,
        tts_volume=tts_volume,
    )


def render_chat_history() -> None:
    """LangGraph ã® state ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ messages ã‚’åˆ—æŒ™ã—ã¦è¡¨ç¤ºã™ã‚‹ã€‚"""
    agent_messages = get_agent_messages()
    for msg in agent_messages:
        role = "assistant"
        content = ""
        attachments = []
        if isinstance(msg, dict):
            role = msg.get("role", role)
            content = msg.get("content", content)
            attachments = msg.get("attachments", []) or []
        else:  # LangChain Message ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆäº’æ›
            role = getattr(msg, "role", role)
            content = getattr(msg, "content", content)
        with st.chat_message(role):
            if attachments:
                for item in attachments:
                    render_attachment(item)
            if content:
                st.write(content)


def render_attachment(item: dict[str, object]) -> None:
    item_type = item.get("type")
    if item_type == "text":
        st.markdown(item.get("text", ""))
    elif item_type == "image_url":
        url = item.get("image_url", {}).get("url")
        if url:
            st.image(url)


def collect_user_submission(mode: str, audio_settings: AudioSettings | None) -> UserSubmission | None:
    if mode == "éŸ³å£°":
        return collect_audio_submission(audio_settings)
    if mode == "ãƒ†ã‚­ã‚¹ãƒˆ":
        return collect_text_submission()
    st.error("ä¸æ˜ãªå…¥å‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰ã§ã™")
    return None


def collect_audio_submission(audio_settings: AudioSettings | None) -> UserSubmission | None:
    if not audio_settings or not audio_settings.audio_bytes:
        return None

    st.audio(audio_settings.audio_bytes, format="audio/wav")
    temp_audio_file_path = _write_temp_audio_file(audio_settings.audio_bytes)

    try:
        with st.spinner("éŸ³å£°ã‚’èªè­˜ä¸­..."):
            stt_wrapper = load_stt_wrapper(audio_settings.whisper_model)
            language_param = (
                None if audio_settings.transcription_language == "auto" else audio_settings.transcription_language
            )
            transcribed_text = stt_wrapper.transcribe(temp_audio_file_path, language=language_param)

        if not transcribed_text:
            st.warning("éŸ³å£°ãŒèªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return None

        st.success(f"éŸ³å£°èªè­˜çµæœ: {transcribed_text}")
        return UserSubmission(
            content=transcribed_text,
            display_items=[{"type": "text", "text": transcribed_text}],
        )
    except Exception as exc:  # noqa: BLE001
        st.error(f"éŸ³å£°èªè­˜ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {exc}")
    finally:
        if temp_audio_file_path and os.path.exists(temp_audio_file_path):
            os.unlink(temp_audio_file_path)

    return None


def _write_temp_audio_file(audio_bytes: bytes) -> str:
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_audio_file:
        temp_audio_file.write(audio_bytes)
        return temp_audio_file.name


def collect_text_submission() -> UserSubmission | None:
    prompt = st.chat_input(
        accept_file="multiple",
        file_type=["png", "jpg", "jpeg", "gif", "webp"],
    )

    if not prompt:
        return None

    raw_text = prompt if isinstance(prompt, str) else getattr(prompt, "text", "") or ""
    prompt_files = [] if isinstance(prompt, str) else (getattr(prompt, "files", []) or [])

    display_items: list[dict[str, object]] = []
    message_parts: list[str] = []

    if raw_text.strip():
        display_items.append({"type": "text", "text": raw_text})
        message_parts.append(raw_text)

    has_unsupported_files = False
    for file in prompt_files:
        if file.type and file.type.startswith("image/"):
            image_item = build_image_attachment(file)
            if image_item:
                display_items.append(image_item)
                message_parts.append(f"![image]({image_item['image_url']['url']})")
        else:
            has_unsupported_files = True

    if has_unsupported_files:
        st.warning("ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä»¥å¤–ã®æ·»ä»˜ã¯ç¾åœ¨ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    content = "\n\n".join(message_parts).strip() or "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚"
    return UserSubmission(content=content, display_items=display_items)


def build_image_attachment(file) -> dict[str, object] | None:
    try:
        image_bytes = file.getvalue()
        base64_image = image_to_base64(image_bytes)
        image_url = f"data:{file.type};base64,{base64_image}"
        return {
            "type": "image_url",
            "image_url": {"url": image_url},
        }
    except Exception as exc:  # noqa: BLE001
        st.warning(f"ç”»åƒã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")
    return None


def render_user_submission(submission: UserSubmission) -> None:
    if submission.display_items:
        for item in submission.display_items:
            render_attachment(item)
    else:
        st.write(submission.content)


def get_agent_messages() -> list:
    """LangGraph ã®ç¾åœ¨ state ã‹ã‚‰ messages ã‚’å–å¾—ã€‚ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºé…åˆ—ã€‚"""
    if "graph" not in st.session_state:
        return []
    try:
        state = st.session_state["graph"].get_state(
            {
                "configurable": {
                    "thread_id": st.session_state.get("thread_id"),
                    "user_id": "user_1",
                },
            },
        )
        values = getattr(state, "values", state)
        if isinstance(values, dict):
            return list(values.get("messages", []) or [])
        return []
    except Exception as exc:  # noqa: BLE001
        logger.debug(f"messages state ã®å–å¾—ã«å¤±æ•—: {exc}")
        return []


def build_graph_messages_with_new_user(user_content: str) -> list:
    """æ—¢å­˜ messages ã«æ–°ã—ã„ user ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã—ãŸãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚"""
    return [*get_agent_messages(), {"role": "user", "content": user_content}]


def invoke_agent(graph_messages: list) -> AgentState:
    return st.session_state["graph"].invoke(
        {
            "messages": graph_messages,
        },
        {
            "callbacks": [
                StreamlitCallbackHandler(st.container()),
                CallbackHandler(),
            ],
            "configurable": {
                "thread_id": st.session_state.get("thread_id"),
            },
        },
    )


def synthesize_audio_if_needed(response_content: str, mode: str, audio_settings: AudioSettings | None) -> None:
    if mode != "éŸ³å£°" or not audio_settings:
        return

    try:
        with st.spinner("éŸ³å£°ã‚’ç”Ÿæˆä¸­ã§ã™..."):
            audio_bytes = TtsWrapper().synthesize_audio(
                text=response_content,
                language=audio_settings.tts_language,
                speed=audio_settings.tts_speed,
                pitch_shift=audio_settings.tts_pitch,
                volume_db=audio_settings.tts_volume,
            )
            st.audio(audio_bytes, format="audio/mp3", autoplay=True)
    except Exception as exc:  # noqa: BLE001
        st.warning(f"éŸ³å£°å‡ºåŠ›ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {exc}")


input_output_mode, audio_settings = build_sidebar()

render_chat_history()

submission = collect_user_submission(input_output_mode, audio_settings)

if submission:
    with st.chat_message("user"):
        render_user_submission(submission)

    updated_messages = build_graph_messages_with_new_user(submission.content)
    with st.chat_message("assistant"):
        response = invoke_agent(updated_messages)
        latest_messages = response["messages"]
        last_message = latest_messages[-1] if latest_messages else None
        if last_message is not None:
            response_content = getattr(last_message, "content", None) or (
                last_message.get("content") if isinstance(last_message, dict) else ""
            )
            st.write(response_content)
            synthesize_audio_if_needed(response_content, input_output_mode, audio_settings)
