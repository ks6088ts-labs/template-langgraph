import os
import tempfile
from base64 import b64encode
from datetime import datetime

import streamlit as st
import whisper
from audio_recorder_streamlit import audio_recorder
from langchain_community.callbacks.streamlit import (
    StreamlitCallbackHandler,
)

from template_langgraph.agents.chat_with_tools_agent.agent import (
    AgentState,
    ChatWithToolsAgent,
)
from template_langgraph.speeches.tts import synthesize_audio
from template_langgraph.tools.common import get_default_tools


def image_to_base64(image_bytes: bytes) -> str:
    return b64encode(image_bytes).decode("utf-8")


@st.cache_resource(show_spinner=False)
def load_whisper_model(model_size: str = "base"):
    """Load a Whisper model only once per session."""

    return whisper.load_model(model_size)


if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# Sidebar: å…¥å‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰é¸æŠã€ãƒ„ãƒ¼ãƒ«é¸æŠã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ§‹ç¯‰
with st.sidebar:
    st.subheader("å…¥å‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰")

    # å…¥å‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰é¸æŠ
    if "input_output_mode" not in st.session_state:
        st.session_state["input_output_mode"] = "ãƒ†ã‚­ã‚¹ãƒˆ"

    input_output_mode = st.radio(
        "ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„",
        options=["ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"],
        index=0 if st.session_state["input_output_mode"] == "ãƒ†ã‚­ã‚¹ãƒˆ" else 1,
        help="ãƒ†ã‚­ã‚¹ãƒˆ: å¾“æ¥ã®ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›/å‡ºåŠ›, éŸ³å£°: ãƒã‚¤ã‚¯å…¥åŠ›/éŸ³å£°å‡ºåŠ›",
    )
    st.session_state["input_output_mode"] = input_output_mode

    # éŸ³å£°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€Whisper è¨­å®šã‚’è¡¨ç¤º
    if input_output_mode == "éŸ³å£°":
        st.subheader("éŸ³å£°èªè­˜è¨­å®š (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")
        audio_bytes = audio_recorder(
            text="ã‚¯ãƒªãƒƒã‚¯ã—ã¦éŸ³å£°å…¥åŠ›ğŸ‘‰ï¸",
            recording_color="red",
            neutral_color="gray",
            icon_name="microphone",
            icon_size="2x",
            key="audio_input",
        )
        selected_model = st.sidebar.selectbox(
            "Whisperãƒ¢ãƒ‡ãƒ«",
            [
                "tiny",
                "base",
                "small",
                "medium",
                "large",
            ],
            index=1,
        )
        transcription_language = st.sidebar.selectbox(
            "æ–‡å­—èµ·ã“ã—è¨€èª",
            [
                "auto",
                "ja",
                "en",
            ],
            index=0,
            help="autoã¯è¨€èªè‡ªå‹•åˆ¤å®šã§ã™",
        )
        tts_language = st.sidebar.selectbox(
            "TTSè¨€èª",
            [
                "ja",
                "en",
                "fr",
                "de",
                "ko",
                "zh-CN",
            ],
            index=0,
        )
        tts_speed = st.sidebar.slider(
            "å†ç”Ÿé€Ÿåº¦",
            min_value=0.5,
            max_value=2.0,
            step=0.1,
            value=1.0,
        )
        tts_pitch = st.sidebar.slider(
            "ãƒ”ãƒƒãƒ (åŠéŸ³)",
            min_value=-12,
            max_value=12,
            value=0,
        )
        tts_volume = st.sidebar.slider(
            "éŸ³é‡ (dB)",
            min_value=-20,
            max_value=10,
            value=0,
        )

    st.divider()
    st.subheader("ä½¿ç”¨ã™ã‚‹ãƒ„ãƒ¼ãƒ«")

    # åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ä¸€è¦§ã‚’å–å¾—
    available_tools = get_default_tools()
    tool_name_to_obj = {t.name: t for t in available_tools}
    tool_names = list(tool_name_to_obj.keys())

    # åˆæœŸé¸æŠã¯å…¨é¸æŠ
    if "selected_tool_names" not in st.session_state:
        st.session_state["selected_tool_names"] = tool_names

    selected_tool_names = st.multiselect(
        "æœ‰åŠ¹åŒ–ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’é¸æŠ",
        options=tool_names,
        default=st.session_state["selected_tool_names"],
    )
    st.session_state["selected_tool_names"] = selected_tool_names

    # é¸æŠã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã§ã‚°ãƒ©ãƒ•ã‚’å†æ§‹ç¯‰ï¼ˆé¸æŠãŒå¤‰ã‚ã£ãŸæ™‚ã®ã¿ï¼‰
    selected_tools = [tool_name_to_obj[name] for name in selected_tool_names]
    signature = tuple(selected_tool_names)
    if "graph" not in st.session_state or st.session_state.get("graph_tools_signature") != signature:
        st.session_state["graph"] = ChatWithToolsAgent(tools=selected_tools).create_graph()
        st.session_state["graph_tools_signature"] = signature
    # é¸æŠä¸­ã®ãƒ„ãƒ¼ãƒ«è¡¨ç¤ºï¼ˆç°¡æ˜“ï¼‰
    st.caption("é¸æŠä¸­: " + (", ".join(selected_tool_names) if selected_tool_names else "ãªã—"))

for msg in st.session_state["chat_history"]:
    if isinstance(msg, dict):
        attachments = msg.get("attachments", [])
        with st.chat_message(msg["role"]):
            if attachments:
                for item in attachments:
                    if item["type"] == "text":
                        st.markdown(item["text"])
                    elif item["type"] == "image_url":
                        st.image(item["image_url"]["url"])
            else:
                st.write(msg["content"])
    else:
        st.chat_message("assistant").write(msg.content)

# å…¥åŠ›ã‚»ã‚¯ã‚·ãƒ§ãƒ³: ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦åˆ†å²
prompt = None
prompt_text = ""
prompt_files = []

if input_output_mode == "éŸ³å£°":
    if audio_bytes:
        st.audio(audio_bytes, format="audio/wav")

        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_audio_file:
            temp_audio_file.write(audio_bytes)
            temp_audio_file_path = temp_audio_file.name
            st.download_button(
                label="ğŸ§ éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜",
                data=audio_bytes,
                file_name=f"recorded_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav",
                mime="audio/wav",
                use_container_width=True,
            )
        try:
            if input_output_mode == "éŸ³å£°":
                with st.spinner("éŸ³å£°ã‚’èªè­˜ä¸­..."):
                    model = load_whisper_model(selected_model)
                    language_param = None if transcription_language == "auto" else transcription_language
                    result = model.transcribe(str(temp_audio_file_path), language=language_param)
                    transcribed_text = result.get("text", "").strip()
                    prompt_text = transcribed_text

                    if prompt_text:
                        st.success(f"éŸ³å£°èªè­˜å®Œäº†: {prompt_text}")
                        prompt = prompt_text
                    else:
                        st.warning("éŸ³å£°ãŒèªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        except Exception as e:
            st.error(f"éŸ³å£°èªè­˜ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            prompt_text = "éŸ³å£°å…¥åŠ›ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
        finally:
            if os.path.exists(temp_audio_file_path):
                os.unlink(temp_audio_file_path)

else:
    # æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ¢ãƒ¼ãƒ‰
    if prompt := st.chat_input(
        accept_file="multiple",
        file_type=[
            "png",
            "jpg",
            "jpeg",
            "gif",
            "webp",
        ],
    ):
        pass  # promptã¯æ—¢ã«è¨­å®šæ¸ˆã¿

# å…±é€šã®å…¥åŠ›å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯
if prompt:
    user_display_items = []
    message_parts = []

    prompt_text = prompt if isinstance(prompt, str) else getattr(prompt, "text", "") or ""
    prompt_files = [] if isinstance(prompt, str) else (getattr(prompt, "files", []) or [])

    user_text = prompt_text
    if user_text.strip():
        user_display_items.append({"type": "text", "text": user_text})
        message_parts.append(user_text)

    has_unsupported_files = False
    for file in prompt_files:
        if file.type and file.type.startswith("image/"):
            image_bytes = file.getvalue()
            base64_image = image_to_base64(image_bytes)
            image_url = f"data:{file.type};base64,{base64_image}"
            user_display_items.append(
                {
                    "type": "image_url",
                    "image_url": {"url": image_url},
                }
            )
            message_parts.append(f"![image]({image_url})")
        else:
            has_unsupported_files = True

    if has_unsupported_files:
        st.warning("ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ä»¥å¤–ã®æ·»ä»˜ã¯ç¾åœ¨ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    message_content = "\n\n".join(message_parts).strip()
    if not message_content:
        message_content = "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚"

    new_user_message = {"role": "user", "content": message_content}
    if user_display_items:
        new_user_message["attachments"] = user_display_items

    st.session_state["chat_history"].append(new_user_message)

    with st.chat_message("user"):
        if user_display_items:
            for item in user_display_items:
                if item["type"] == "text":
                    st.markdown(item["text"])
                elif item["type"] == "image_url":
                    st.image(item["image_url"]["url"])
        else:
            st.write(message_content)

    graph_messages = []
    for msg in st.session_state["chat_history"]:
        if isinstance(msg, dict):
            graph_messages.append({"role": msg["role"], "content": msg["content"]})
        else:
            graph_messages.append(msg)

    with st.chat_message("assistant"):
        response: AgentState = st.session_state["graph"].invoke(
            {"messages": graph_messages},
            {
                "callbacks": [
                    StreamlitCallbackHandler(st.container()),
                ]
            },
        )
        last_message = response["messages"][-1]
        st.session_state["chat_history"].append(last_message)

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¡¨ç¤ºã¨ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå‡ºåŠ›
        response_content = last_message.content
        st.write(response_content)

        # éŸ³å£°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€éŸ³å£°å‡ºåŠ›ã‚’è¿½åŠ 
        if input_output_mode == "éŸ³å£°":
            try:
                with st.spinner("éŸ³å£°ã‚’ç”Ÿæˆä¸­ã§ã™..."):
                    audio_bytes = synthesize_audio(
                        text=response_content,
                        language=tts_language,
                        speed=tts_speed,
                        pitch_shift=tts_pitch,
                        volume_db=tts_volume,
                    )
                    st.audio(audio_bytes, format="audio/mp3", autoplay=True)
            except Exception as e:
                st.warning(f"éŸ³å£°å‡ºåŠ›ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
